# Optimizing an ML Pipeline in Azure

## Overview
This project is part of the Udacity Azure ML Nanodegree.
In this project, we build and optimize an Azure ML pipeline using the Python SDK and a provided Scikit-learn model.
This model is then compared to an Azure AutoML run.

## Summary
This dataset contains data about bank marketing campaigns, it's a binary classfication exercise to predict if the client will subscribe to a term deposit with the bank. One starts to create and optimize an Scikit Learn Logistic Regression ML pipeline with the HyperDrive tool to optimize the hyperparamaters.
Then one will be comparing with the Auto ML results.

The Hyperdrive best performance was achieved with a pipeline Logistic Regression (0.9170), 57s.
The AutoML best performance was achieved with a pipeline MaxAbsScaler LightGBM (0.9152), first iteration 31s.

## Scikit-learn Pipeline
**Explain the pipeline architecture, including data, hyperparameter tuning, and classification algorithm.**

The pipeline architecture is the following:

![alt text](https://github.com/Jocker1980/AzureML_Engineer_Nanodegree/blob/main/Images/ml-pipeline.png)

A TabularDatasetFactory have been create out of a web data set file, then it is cleaned and pass to algorithms to build the best predictive classification model.
The aim of the exercise was to compare the tools HyperDrive and AutoML from MS Azure ML Studio for a Classification problem.
The HyperDrive let the MLOps engineer fine tune the hyperparameter search technique and parameter space.
The AutoML is straight forward and allow a beginner MLOps to save time and find the best configuration.

**What are the benefits of the parameter sampler you chose?**
The hyperparameter sampler selected is RandomParameterSampler.
A Random Search offer the benefit of the speed with a result often very close to the Grid Search who explore the complete parameter space.

**What are the benefits of the early stopping policy you chose?**
The Early Stopping Policy selected is BanditPolicy. 
An early stopping policy is used to automatically terminate poorly performing runs to improve computational efficiency.

## AutoML
**In 1-2 sentences, describe the model and hyperparameters generated by AutoML.**

## Pipeline comparison
**Compare the two models and their performance. What are the differences in accuracy? In architecture? If there was a difference, why do you think there was one?**
Steps for the HyperDrive:
Create a TabularDatasetFactory out of a web data set file.
Then that data set have been cleaned through a python script and its data_clean() function.
The data set is then splited in train/test sub data sets.
The algorithm train a Logistic Regression model on the train data set and optimize the hyperparameter with HyperDrive to maximize the metric selected (accuracy).

On the other side with the AutoML I repeated the same data preparation except the data set vas not splitted as the AutoML will use cross validation to train the models.
The AutoML configuration include the type of model to use, here classification, the metric accuracy, the data set and the CV parameter n-cross_validation=5. This approach is more high level and allow a beginner to benefit from the power of the computing of multi configuration testing.

In terme of architecture, one can see that the HyperDrive was having its configuration space limited by the imposed model type and by the skills of the MLOps in his/her choise of parameters space. On the other side AutoML is optimized.

Model performances:
The Hyperdrive best performance was achieved with a pipeline Logistic Regression (0.9170), 57s.
The AutoML best performance was achieved with a pipeline MaxAbsScaler LightGBM (0.9152), first iteration 31s.
The difference on the accuracy metrics is negligeable. However this metric certainly do not shows all the performances difference. It would be good to select some complementray metrics.

## Future work
**What are some areas of improvement for future experiments? Why might these improvements help the model?**
The data set is higly imbalanced that is an issue to get the best model when one uses HyperDrive. Using specific package to overcome this for HyperDrive. AutoML can detect and handle imbalanced data sets.

I did not see in the HyperDrive architecture, the data standardization step.
It is not clear to me for the moment how HyperDrive or AutoML assess the overfitting.

In my opinion the part where a MLOps can really do the difference is in the feature engineering steps. By understanding more the contexte of the data one can be creative and generate new features, that are out of automated mathematical transforamtions. Domain knowledge expertise often make the difference.

## Proof of cluster clean up
![alt text](https://github.com/Jocker1980/AzureML_Engineer_Nanodegree/blob/main/Images/Compute_target_del.PNG)

